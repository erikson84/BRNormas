```{r, include=FALSE}
library(ggplot2)
library(gridExtra)
library(knitr)
library(kableExtra)
```

# Fundamentação estatística para normas baseadas em regressão

## Dificuldades em estabelecer medidas em Psicologia

Na Física e nas demais ciências naturais, uma *medida* é definida pelo produto de uma *magnitude* de um atributo quantitativo com uma unidade fundamental de referência. É crucial que a relação entre diferentes magnitudes, como proposta por equações de modelos científicos, seja independente da unidade fundamental utilizada para expressar a medida. Essa *invariância* com relação à unidade de referência utilizada para expressar uma medida só é possível quando a operacionalização do processo de mensuração do atributo quantitativo resultar em valores que permitam a operação de divisão, i.e., ser possível e significativo calcular *razões* entre duas medidas.

Nas Psicologia, via de regra, esse nível de mensuração não costuma ser proposto ou mesmo tentado. Em seu lugar, a Psicologia aceitou amplamente a noção *operacionalista* de mensuração, na qual um atributo quantitativo é definido pelo processo estipulado para sua mensuração. Nesse contexto, uma medida é simplesmente um conjunto de regras que permite atribuir valores numéricos a atributos, independente de sua natureza. Apesar de o operacionalismo permitir ampliar a noção de medida e justificar *a priori* a possibilidade de mensuração de qualquer construto psicológico, os valores numéricos obtidos pela aplicação de regras arbitrárias de mensuração não mantém a estrutura matemática necessária para a realização de operações básicas. Dessa forma, não é possível estabelecer razões entre diferentes medidas e, portanto, não faz sentido tentar estabelecer uma unidade fundamental de referência.

## O papel das normas estatísticas

Mas nem tudo está perdido para as mensurações possíveis de serem obtidas por meio de instrumentos psicológicos. Na ausência de uma unidade fundamental de referência que dê sentido às magnitudes obtidas, a Psicologia propõe a comparação dos valores resultantes da avaliação de um sujeito numa determinada medida definida por um instrumento psicológico à *distribuição de probabilidade* dessa medida em uma população de referência, chamada de *população normativa*. A Psicologia substitui, então, a necessidade de uma unidade de medida por um critério estatístico de comparação, permitindo pelo menos o ordenamento de diferentes sujeitos com relação ao atributo de interesse (ou, mais especificamente, aos valores obtidos pelo processo de mensuração associado a um construto postulado).

Essa comparação da medida de um sujeito à população normativa pode ser expressa de diversas maneiras, mas, a título de generalidade, vamos supor que o ordenamento permita, minimamente, estabelecer o *percentil* ao qual o sujeito avaliado pertence. O *percentil* é definido como a probabilidade (aqui também possível de ser entendida como proporção) de uma variável aleatória *X* ser inferior ou igual a um valor fixo *x*.

$$\mathrm{Percentil}(x) = \mathrm{Pr}(X \leq x)$$

Em uma situação ideal, se conseguimos observar todos os valores de um população normativa finita, e supondo que o processo de mensuração atribua números pertencentes a um conjunto ordenado e finito (ou pelo menos contável), a distribuição de probabilidade de referência pode ser expressa como um histograma e o percentil de um sujeito pode ser facilmente computado calculando a proporção entre os casos com valores menores ou iguais ao valor observado e o tamanho total da população.

:::: {.examplebox data-latex=""}
::: {.center data-latex=""}

**Exemplo**

:::

Um item com enunciado "Eu me sinto disposto ao acordar" é avaliado em uma escala ordinal com cinco alternativas ordenadas -- "Discordo Fortemente, "Discordo", "Nem concordo, nem discordo", "Concordo", "Concordo Fortemente" -- codificados como números inteiros dentro do intervalo [1, 5]. Vamos supor que a população normativa possua 700 pessoas e o item tenha sido aplicado em todos os sujeitos, resultando na seguinte distribuição de frequência:

```{r}
samp_data <- data.frame(Valor=1:5,
                        Frequência=c(60, 150, 260, 190, 40))
kable(samp_data, booktabs=T, caption='Histograma de frequência dos valores numa população finita')
```

Nesse exemplo idealizado, há apenas cinco valores possíveis para os percentis: quem obtém escore igual a um pertence ao percentil 8,6% ($\frac{60}{700}$); quem obtém escore igual a dois pertence ao percentil 30% ($\frac{60 + 150}{700}$); quem obtém escore igual a três pertence ao percentil 67,1% ($\frac{60 + 150 + 260}{700}$); quem obtém escore igual a quatro pertence ao percentil 94,3% ($\frac{60 + 150 + 260 + 190}{700}$); e, por fim, quem obtém escore igual a cinco pertence ao percentil 100% ($\frac{60 + 150 + 260 + 190 + 40}{700}$).

O uso do percentil como medida de comparação entre a resposta de um sujeito e a população normativa também pode ser visualizado graficamente por meio de um histograma. Na figura abaixo, um sujeito que escolheu a alternativa "Concordo", codificada com valor quatro, tem sua posição com relação à população identificada por meio da linha vertical tracejada. Toda a população que escolheu um valor igual ou menor desse sujeito está representada pelas barras em cor azul; o restante da população está representada pela barra em cor vermelha. O percentil é computado pela soma do número de respondentes em cada barra em cor azul dividida pelo tamanho total da população.

```{r, fig.align='center', fig.cap='Histograma de Frequência de respostas na população'}
ggplot(samp_data, aes(x=Valor, y=Frequência)) +
  geom_bar(stat='identity', fill=ifelse(samp_data$Valor <= 4, 'steelblue', 'maroon')) +
  geom_vline(xintercept=4, linetype=2)
```

::::

## Dois desafios: aproximação e estimação

Para muitas medidas -- especialmente aquelas derivadas da soma ou média de diversos itens de um instrumento -- o número resultante da operação de mensuração pode variar em muitos níveis e, mesmo que esse níveis sejam finitos e não estritamente contínuos, podemos tratar a medida como propriamente contínua. Mesmo que, teoricamente, algum instrumento pudesse produzir resultados contínuos, o fato de as populações serem invariavelmente finitas implica que só um número finito de possibilidades podem ser efetivamente observado. Ainda assim, pode ser frutífero considerar a medida como contínua e avaliar seu percentil a partir de uma função de distribuição acumulada.

O desafio de suavizar a função de distribuição acumulada de uma população finita -- supondo que ela pode ser observada por completo -- traz um primeiro desafio de como melhor *aproximar* uma distribuição descontínua por uma função contínua. Essa questão, todavia, torna-se secundária quando é impossível observar a população normativa por completo -- o que é mais comumente a regra do que a exceção. Nesse caso, o segundo e principal desafio é como *estimar* a função de distribuição acumulada tendo acesso exclusivamente a uma amostra da população normativa.

Ambos os desafios costumam ser resolvidos de uma só vez por meio da adoção de um *modelo* estatístico para descrever a população. Mesmo que a população e os valores produzidos pelo processo de mensuração sejam finitos, a população é modelada por uma função de distribuição de probabilidade com suporte contínuo. Tradicionalmente, é escolhida uma função de probabilidade que pode ser descrita por um número finito de parâmetros; em particular, a distribuição Gaussiana ou Normal, indexada apenas pelos parâmetros de média e variância, é comumente empregada para descrever a população e indicar os parâmetros de interesse para obtenção da função de distribuição acumulada^[A literatura estatística apresenta alternativas variadas para a aproximação da função de distribuição acumulada, desde o uso da distribuição acumulada empírica para os valores obtidos para a amostra -- que pode ser bastante ruidoso, especialmente nas caudas -- até estratégia de suavização não-paramétricas baseadas em *kernels* gaussianos ou em processos de Dirichlet. Como meio caminho existe a possibilidade de utilizar formas funcionais de famílias paramétricas, supondo que a família representa adequadamente a distribuição da população.].

:::: {.examplebox data-latex=""}
::: {.center data-latex=""}

**Exemplo**

:::

Um teste de inteligência geral com 20 itens é aplicado em uma população de 500 estudantes. Por definição, os escores derivados deste teste são discretos: somente 21 valores são possíveis. Além disso, a própria população de referência é finita, formada por 500 pessoas. A função de distribuição cumulativa, calculada de forma empírica para todas as observações e suavizadas pela aproximação gaussiana, é apresentada na Figura 2. É importante destacar que a utilização da aproximação Gaussiana nesse caso cumpre exclusivamente a função de *aproximação* e *suavização* da função empírica.

A figura também apresenta três estimativas dessa função de distribuição acumulada com base em amostras aleatórias simples de 30 sujeitos dessa população. Nesse caso, a aproximação Gaussiana cumpre o duplo papel mencionado acima: simultaneamente, ela permite *aproximar* a distribuição da população e facilita a *estimação* da função populacional por meio das estatísticas calculadas a partir da amostra.

```{r, fig.cap='Distribuição acumulada dos escores de QI na população e em três amostras aleatórias'}
pop_data <- round(rnorm(500, 10, 2))

plot_pop <- ggplot(data.frame(QI=pop_data), aes(x=QI)) + stat_ecdf(aes(color='Dist. Emp.')) + 
  geom_function(fun=function(x) pnorm(x+0.5, mean(pop_data), sd(pop_data)), aes(color='Suav. Gaussiana')) +
  labs(y='Pr(QI <= x)', color='Legenda') + 
  scale_color_manual(values=c('Dist. Emp.'='steelblue', 'Suav. Gaussiana'='maroon'))

plot_list <- list(pop=plot_pop)

for (pl in c('samp1', 'samp2', 'samp3')) {
  samp <- sample(pop_data, 30)
  plot_df <- data.frame(QI=samp)
  plot_samp <- ggplot(plot_df, aes(x=QI)) + stat_ecdf(color='steelblue') + 
    geom_line(data=data.frame(x=seq(4, 16, length.out=100), y=pnorm(seq(4, 16, length.out=100), mean(samp), sd(samp))),
              aes(x=x, y=y), color='maroon') + 
    geom_function(fun=function(x) pnorm(x+0.5, mean(pop_data), sd(pop_data)), color='black', linetype=2) +
    labs(y='Pr(QI <= x)') + theme(legend.position = 'none') + xlim(3, 16)
  
  plot_list[[pl]] <- plot_samp

}

grid.arrange(grobs=plot_list, layout_matrix=rbind(c(1, 1, 1), c(2, 3, 4)))


```
Qual a vantagem de utilizar a aproximação Gaussiana neste exemplo? Para os dados da população completa, a vantagem é meramente de compressão de dados: em vez de armazenar 500 observações, podemos obter a mesma precisão com apenas dois números: a média e a variância -- assumindo, é claro, que a forma funcional é adequada para aproximar a distribuição da população.

No caso de utilizar a aproximação Gaussiana para facilitar a estimação da função de distribuição acumulada, há uma segunda vantagem -- também condicional à boa aproximação da forma funcional pela distribuição Normal: o erro no percentil obtido a partir da aproximação normal será menor do que o percentil obtido pela função de distribuição acumulada empírica, especialmente para amostras menores. Isso fica evidente nos gráficos acima: a curva estimada pela aproximação Gaussiana (em vermelho escuro) se aproxima da curva baseada nos parâmetros populacionais (em preto tracejado); a aproximação por meio da função de distribuição acumulada empírica (em azul) apresenta maior variação e, portanto, maior erro.

::::

## Populações heterogêneas

Se a população normativa de um escala psicológica é relativamente homogênea ou se o atributo mensurado não varia sistematicamente em função de outras variáveis, uma única tabela normatiza é suficiente. Porém, esse nível de homogeneidade raramente é observado para variáveis psicológicas: elas são afetadas por diversos fatores com diferentes graus de sistematicidade. Nesses casos, uma única referência normativa pode ser inadequada, a depender dos objetivos propostos para a avaliação.

No caso de populações heterogêneas, precisamos identificar as fontes relevantes de variação e elaborar uma tabela normativa para cada nível pertinente dessas fontes. Em outras palavras, precisamos encontrar a *função de distribuição acumulada condicional* da medida de interesse em função dos diferentes valores das variáveis influenciadoras.

$$\mathrm{FDA}(x \mid Y=y, \dots, Z=z) = \mathrm{Pr}(X \leq x \mid Y=y, \dots, Z=z)$$
A maioria dos testes psicológicos elege um pequeno número de variáveis sociodemográficas consideradas relevantes e elabora uma tabela normativa para cada um dos níveis dessas variáveis, geralmente de forma separada (i.e.: sem considerar o efeito da interação de níveis dos preditores). Apesar de ser um avanço sobre a apresentação de uma única tabela normativa, as limitações desse procedimento são óbvias:

1. Só é possível escolher variáveis discretas ou discretizar em classes amplas variáveis contínuas; 
2. É difícil considerar como condicionante mais de uma variável por vez, levando a construção de tabelas, p.ex., para gênero ou escolaridade mas não para gênero e escolaridade simultaneamente; 
3. O cálculo dos valores dessa tabela, se feito de forma exclusivamente empírica, subdividindo a amostra de acordo com os níveis das variáveis condicionantes, terá um número menor de pontos de dados e será mais ruidoso.

De maneira geral, o problema estatístico de estimar valores para um *desfecho* de interesse (em nosso caso, uma medida psicológica), que passaremos a representar como *y*, em função dos valores condicionais de um ou mais *preditores* (como variáveis sociodemográficas ou outras pertinentes ao atributo avaliado), que representaremos pelo vetor **x**, é endereçado pelos modelos de *regressão*. A maioria dos modelos de regressão buscar estimar o valor esperado do desfecho condicionado nos níveis observados dos preditores. Em particular, os modelos de *regressão linear* tentam aproximar essa função pela combinação aditiva dos preditores, ponderados por coeficientes de inclinação ($\boldsymbol{\beta}$, na equação abaixo).

$$\mathbb{E}(y_i\mid\textbf{x}_i, \boldsymbol{\beta})=\alpha + \beta_1x_{i1}+\dots+\beta_kx_{ik}$$

### Regressão linear

Via de regra, a distribuição condicional de probabilidade $p(y\mid\boldsymbol{x})$ não é conhecida e as dependências entre a variável alvo e as variáveis condicionantes podem ser mais complexas do que é possível capturar com um modelo estatístico paramétrico. Mesmo assim, uma aproximação inicial pode ser obtida se for plausível assumir algumas simplificações:

1. A variação média de um desfecho em função da variação de um preditor é constante e não depende do valor específico do preditor -- esse pressuposto implica na *linearidade* da relação entre desfecho e preditores, e pode ser flexibilizado pelo acréscimo de polinômios dos preditores com relação não-linear;
2. A maneira como um preditor influencia o desfecho não depende dos valores dos outros preditores -- esse pressuposto implica da *aditividade* dos preditores, e pode ser flexibilizado pelo acréscimo de combinações (*interações*) entre preditores com influência mútua;
3. Os valores mensurados para os preditores não possuem erro e podem ser tratados como fixos, ou, minimamente, os erros nos preditores podem ser desprezados sem consequência para as predições -- esse pressuposto de *exogeneidade fraca* implica que não precisamos considerar o modelo da distribuição conjunta de todas as variáveis;
4. A maneira como o valor observado difere do valor esperado predito pelo modelo -- os *erros* -- não depende nem do valor predito nem dos valores específicos dos preditores -- esse pressuposto determina a *independência dos erros* e não pode ser flexibilizado sem mudar fundamentalmente a estrutura do modelo (transformando-o, p.ex., em um modelo hierárquico para dar conta de dependência de erros entre grupos);
5. A variância dos erros é constante para qualquer valor predito e independe dos valores do preditores -- esse pressuposto implica na *homogeneidade da variância* dos erros do modelo e é assumido por algoritmos tradicionais, como mínimos quadrados e máxima verossimilhança, mas pode ser flexibilizado utilizando mínimos quadrados ponderados ou estimadores robustos para a Hessiana do modelo de máxima verossimilhança;
6. A *distribuição dos erros* segue uma função de densidade gaussiana -- esse pressuposto assume a *normalidade dos erros* e só é necessário porque nossa tarefa envolve calcular a função de distribuição acumulada para avaliar os percentis normativos e não é, de forma alguma, um pressuposto para a regressão linear para fins de predição do valor esperado do desfecho.

Mesmo que não disponhamos de informações *a priori* para avaliar se as simplificações assumidas pelos pressupostos são válidas para aproximarmos a distribuição condicional pelo modelo de regressão linear, o modelo ajustado pode ser verificado por meio de análises específicas que permitem diagnosticar a adequação razoável ou identificar desvios graves desse pressupostos. Mas antes de passar às questões de checagem de ajuste, vamos avaliar melhor os elementos principais de um modelo de regressão linear.

